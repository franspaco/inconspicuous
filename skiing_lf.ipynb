{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from RunHist import RewardHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_reward(I, prev):\n",
    "    I = I[:, :, 1]\n",
    "    I = I[74:75, 8:152]  # Jugador 92, bandera roja 50, bandera azul 72\n",
    "    if 72 not in I and 50 not in I:\n",
    "        return 0\n",
    "    if 72 in I:\n",
    "        flags = np.where(I == 72)\n",
    "    else:\n",
    "        flags = np.where(I == 50)\n",
    "\n",
    "    player = np.where(I == 92)[1]\n",
    "\n",
    "    if len(player) == 0:\n",
    "        return 1\n",
    "\n",
    "    player = player.mean()\n",
    "\n",
    "    if len(flags[1]) == 2:\n",
    "        if player >= flags[1][0] and player <= flags[1][1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return prev\n",
    "\n",
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0:\n",
    "            # if the game ended (in Pong), reset the reward sum\n",
    "            running_add = 0\n",
    "        # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r)  # normalizing the result\n",
    "    discounted_r /= np.std(discounted_r)  # idem\n",
    "    return discounted_r\n",
    "\n",
    "#other code\n",
    "def get_pos_player(observe):\n",
    "  ids = np.where(np.sum(observe == [214, 92, 92], -1) == 3)\n",
    "  return ids[0].mean(), ids[1].mean()\n",
    "\n",
    "def get_pos_flags(observe):\n",
    "  if np.any(np.sum(observe == [184, 50, 50], -1) == 3):\n",
    "    ids = np.where(np.sum(observe == [184, 50, 50], -1) == 3)\n",
    "    return ids[0].mean(), ids[1].mean()\n",
    "  else:\n",
    "    base = 0\n",
    "    ids = np.where(np.sum(observe[base:-60] == [66, 72, 200], -1) == 3)\n",
    "    return ids[0].mean() + base, ids[1].mean()\n",
    "\n",
    "def get_speed(observe, observe_old):\n",
    "  min_val = np.inf\n",
    "  min_idx = 0\n",
    "  for k in range(0, 7):\n",
    "    val = np.sum(np.abs(observe[54:-52,8:152] - observe_old[54+k:-52+k,8:152]))\n",
    "    if min_val > val:\n",
    "      min_idx = k\n",
    "      min_val = val\n",
    "  return min_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skier:\n",
    "    def __init__(self, gamma=0.95, epsilon=1, e_min=0.05, e_decay=0.99, ideal_flag_interval=25):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = e_min\n",
    "        self.epsilon_decay = e_decay\n",
    "\n",
    "        self.episode = 0\n",
    "\n",
    "        self.ideal_flag_interval = ideal_flag_interval\n",
    "\n",
    "        self.autosave = None\n",
    "\n",
    "        self.model = self._make_model()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _make_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(146, 144, 2)))\n",
    "        model.add(layers.Conv2D(8, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units=512,activation='relu'))\n",
    "        model.add(layers.Dense(units=3,activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def preprocessFrame(self, I):\n",
    "        \"\"\" \n",
    "        Outputs a 72x72 image where background is black\n",
    "        and important game elements are white.\n",
    "        Output is [0,1]\n",
    "        \"\"\"\n",
    "        I = I[57:203, 8:152, 1]\n",
    "        return I/255\n",
    "\n",
    "    def decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_reward = 0\n",
    "        self.rewards = []\n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        self.lr_counter = 0\n",
    "        self.missing_flags = 20\n",
    "        self.last_frame = np.zeros((146, 144))\n",
    "\n",
    "    def action(self, frame, training=False):\n",
    "        frame = self.preprocessFrame(frame)\n",
    "        data = np.zeros((146, 144, 2))\n",
    "        data[:,:,0] = self.last_frame\n",
    "        data[:,:,0] = frame\n",
    "        self.last_frame = frame\n",
    "        probs = self.model.predict(np.expand_dims(data, 0))\n",
    "        #y = np.random.choice([0, 1, 2], p=probs[0])\n",
    "        y = np.argmax(probs[0])\n",
    "        print(probs[0], end='\\r')\n",
    "        \n",
    "        if float('nan') in probs[0]:\n",
    "            print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "            exit()\n",
    "\n",
    "        # Append flattened frame to x_train\n",
    "        self.train_x.append(data)\n",
    "        # Append selected action to y_train\n",
    "        self.train_y.append(to_categorical(y, num_classes=3))\n",
    "        return y\n",
    "\n",
    "    def register_frame(self, frame, rew = 0):\n",
    "        frame_reward = get_frame_reward(frame, self.last_reward)\n",
    "        reward = rew\n",
    "        self.lr_counter += 1\n",
    "        if frame_reward == 0 and self.last_reward != 0:\n",
    "            reward += self.last_reward + 2\n",
    "            reward -= 0.25 * np.tanh((0.05 * (self.lr_counter - self.ideal_flag_interval)))\n",
    "            self.lr_counter = 0\n",
    "            self.missing_flags -= 1\n",
    "            print(reward, end='\\r')\n",
    "        self.last_reward = frame_reward\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def done(self):\n",
    "        self.rewards[-1] -= self.missing_flags * 5\n",
    "\n",
    "    def train(self, verbose=0):\n",
    "        if self.autosave is not None and self.episode % self.autosave == 0:\n",
    "            self.save(\"last_lf.h5\")\n",
    "            print(\"Saved!\")\n",
    "        \n",
    "        #print(\"missed:\", self.missing_flags, \"flags\")\n",
    "        #self.rewards[-1] -= self.missing_flags * 5\n",
    "        sample_weights = discount_rewards(self.rewards, self.gamma)\n",
    "        self.model.fit(\n",
    "            x=np.array(self.train_x),\n",
    "            y=np.vstack(self.train_y),\n",
    "            verbose=verbose,\n",
    "            sample_weight=sample_weights\n",
    "        )\n",
    "        self.episode += 1\n",
    "        self.decay()\n",
    "\n",
    "    def total_reward(self):\n",
    "        return np.array(self.rewards).sum()\n",
    "\n",
    "    def set_autosave(self, interval):\n",
    "        self.autosave = interval\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 144, 142, 8)       152       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 142, 140, 8)       584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 71, 70, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 69, 68, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 67, 66, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 33, 33, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 17424)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               8921600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 8,927,363\n",
      "Trainable params: 8,927,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Skier(gamma=0.99, e_decay=0.995)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8945252486874986\n",
      "# - = - = - = - #\n",
      "Ep:  227\n",
      "Total reward: 57.330\n",
      "Epsilon: 0.3205\n",
      "Reward AVG:    57.33 | ▲    57.33\n",
      "Best: 57.33012044584222\n",
      "Epoch 1/1\n",
      "560/560 [==============================] - 1s 1ms/step - loss: -0.4521 - acc: 0.7964\n",
      "2.9387703343990728\n",
      "# - = - = - = - #\n",
      "Ep:  228\n",
      "Total reward: 57.389\n",
      "Epsilon: 0.3189\n",
      "Reward AVG:    57.36 | ▲     0.03\n",
      "Best: 57.38923569145897\n",
      "Epoch 1/1\n",
      "557/557 [==============================] - 1s 1ms/step - loss: -0.3821 - acc: 0.7953\n",
      "2.8748699472024413\n",
      "# - = - = - = - #\n",
      "Ep:  229\n",
      "Total reward: -79.597\n",
      "Epsilon: 0.3173\n",
      "Reward AVG:    11.71 | ▼   -45.65\n",
      "Best: 57.38923569145897\n",
      "Epoch 1/1\n",
      "268/268 [==============================] - 0s 1ms/step - loss: -1.3725 - acc: 0.7351\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:  230\n",
      "Total reward: -100.000\n",
      "Epsilon: 0.3157\n",
      "Reward AVG:   -16.22 | ▼   -27.93\n",
      "Best: 57.38923569145897\n",
      "Saved!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast ufunc subtract output from dtype('float64') to dtype('int32') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-360cad944c17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n\u001b[0;32m     97\u001b[0m             \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-107464c253a6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m#print(\"missed:\", self.missing_flags, \"flags\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m#self.rewards[-1] -= self.missing_flags * 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscount_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         self.model.fit(\n\u001b[0;32m    102\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6ec0e872c3d8>\u001b[0m in \u001b[0;36mdiscount_rewards\u001b[1;34m(r, gamma)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mrunning_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_add\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdiscounted_r\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_add\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdiscounted_r\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscounted_r\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# normalizing the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mdiscounted_r\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscounted_r\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# idem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdiscounted_r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast ufunc subtract output from dtype('float64') to dtype('int32') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "\n",
    "#agent.load(\"last.h5\")\n",
    "\n",
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "hist = RewardHist(100)\n",
    "agent.reset()\n",
    "\n",
    "observation_old = observation\n",
    "\n",
    "#other code\n",
    "\n",
    "cnt = 0\n",
    "r_a, c_a = get_pos_player(observation)\n",
    "r_f, c_f = get_pos_flags(observation)\n",
    "r_a_old, c_a_old = r_a, c_a\n",
    "#------------\n",
    "\n",
    "env_reward = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    #action = agent.action(observation, training=True)\n",
    "    \n",
    "\n",
    "    #other code\n",
    "    # TEACHER\n",
    "    v_f = np.arctan2(r_f - r_a, c_f - c_a) # direction from player to target\n",
    "    spd = get_speed(observation, observation_old)\n",
    "    v_a = np.arctan2(spd, c_a - c_a_old) # speed vector of the player\n",
    "    r_a_old, c_a_old = r_a, c_a\n",
    "    \n",
    "    \n",
    "    \n",
    "    frame = agent.preprocessFrame(observation)\n",
    "    data = np.zeros((146, 144, 2))\n",
    "    data[:,:,0] = agent.last_frame\n",
    "    data[:,:,1] = frame\n",
    "    agent.last_frame = frame\n",
    "        \n",
    "    \n",
    "    observation_old = observation\n",
    "    if np.random.rand() <= agent.epsilon:\n",
    "        if spd == 0 and (c_a - c_a_old) == 0:\n",
    "            cnt += 1\n",
    "            act_t = np.random.choice(3, 1)[0]\n",
    "        else:\n",
    "            cnt = 0\n",
    "            if v_f - v_a < -0.1:\n",
    "                act_t = 1\n",
    "            elif v_f - v_a > 0.1:\n",
    "                act_t = 2\n",
    "            else:\n",
    "                act_t = 0\n",
    "\n",
    "    else:\n",
    "        if spd == 0 and (c_a - c_a_old) == 0:\n",
    "            cnt += 1\n",
    "            act_t = np.random.choice(3, 1)[0]\n",
    "        else:\n",
    "            cnt = 0\n",
    "            probs = agent.model.predict(np.expand_dims(data, 0))\n",
    "            act_t = np.argmax(probs[0])\n",
    "\n",
    "            if float('nan') == probs[0][0]:\n",
    "                print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "                exit()\n",
    "\n",
    "\n",
    "    \n",
    "    # Append flattened frame to x_train\n",
    "    agent.train_x.append(data)\n",
    "    # Append selected action to y_train\n",
    "    agent.train_y.append(to_categorical(act_t, num_classes=3))\n",
    "    \n",
    "    observation, reward, done, info = env.step(act_t)\n",
    "    r_a, c_a = get_pos_player(observation)\n",
    "    r_f, c_f = get_pos_flags(observation)\n",
    "    #------------\n",
    "\n",
    "    \n",
    "    \n",
    "    if cnt > 10:\n",
    "        done = True\n",
    "        agent.register_frame(observation,-100.0)\n",
    "    else:\n",
    "        agent.register_frame(observation)\n",
    "    \n",
    "    if done:\n",
    "        #agent.done()\n",
    "        total_reward = agent.total_reward()\n",
    "        hist.add(total_reward)\n",
    "\n",
    "        if agent.episode % 1 == 0:\n",
    "            print('\\n# - = - = - = - #')\n",
    "            print(\n",
    "                f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n",
    "            hist.report()\n",
    "        agent.train(verbose=1)\n",
    "        agent.reset()\n",
    "\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"last_lf_best.h5\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#agent.load(\"last.h5\")\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    \n",
    "    \n",
    "    frame = agent.preprocessFrame(observation)\n",
    "    data = np.zeros((146, 144, 2))\n",
    "    data[:,:,0] = agent.last_frame\n",
    "    data[:,:,0] = frame\n",
    "    agent.last_frame = frame\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    cnt = 0\n",
    "    probs = agent.model.predict(np.expand_dims(data, 0))\n",
    "    act_t = np.argmax(probs[0])\n",
    "\n",
    "    if float('nan') in probs[0]:\n",
    "        print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "        exit()\n",
    "    \n",
    "    observation, reward, done, info = env.step(act_t)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
