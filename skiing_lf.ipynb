{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_reward(I, prev):\n",
    "    I = I[:, :, 1]\n",
    "    I = I[74:75, 8:152]  # Jugador 92, bandera roja 50, bandera azul 72\n",
    "    if 72 not in I and 50 not in I:\n",
    "        return 0\n",
    "    if 72 in I:\n",
    "        flags = np.where(I == 72)\n",
    "    else:\n",
    "        flags = np.where(I == 50)\n",
    "\n",
    "    player = np.where(I == 92)[1]\n",
    "\n",
    "    if len(player) == 0:\n",
    "        return 1\n",
    "\n",
    "    player = player.mean()\n",
    "\n",
    "    if len(flags[1]) == 2:\n",
    "        if player >= flags[1][0] and player <= flags[1][1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return prev\n",
    "\n",
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0:\n",
    "            # if the game ended (in Pong), reset the reward sum\n",
    "            running_add = 0\n",
    "        # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    \n",
    "\n",
    "    discounted_r -= np.mean(discounted_r)  # normalizing the result\n",
    "    discounted_r /= np.std(discounted_r)  # idem\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "#other code\n",
    "def get_pos_player(observe):\n",
    "  ids = np.where(np.sum(observe == [214, 92, 92], -1) == 3)\n",
    "  return ids[0].mean(), ids[1].mean()\n",
    "\n",
    "def get_pos_flags(observe):\n",
    "  if np.any(np.sum(observe == [184, 50, 50], -1) == 3):\n",
    "    ids = np.where(np.sum(observe == [184, 50, 50], -1) == 3)\n",
    "    return ids[0].mean(), ids[1].mean()\n",
    "  else:\n",
    "    base = 0\n",
    "    ids = np.where(np.sum(observe[base:-60] == [66, 72, 200], -1) == 3)\n",
    "    return ids[0].mean() + base, ids[1].mean()\n",
    "\n",
    "def get_speed(observe, observe_old):\n",
    "  min_val = np.inf\n",
    "  min_idx = 0\n",
    "  for k in range(0, 7):\n",
    "    val = np.sum(np.abs(observe[54:-52,8:152] - observe_old[54+k:-52+k,8:152]))\n",
    "    if min_val > val:\n",
    "      min_idx = k\n",
    "      min_val = val\n",
    "  return min_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skier:\n",
    "    def __init__(self, gamma=0.95, epsilon=0.05, e_min=0.05, e_decay=0.99, ideal_flag_interval=25):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = e_min\n",
    "        self.epsilon_decay = e_decay\n",
    "\n",
    "        self.episode = 0\n",
    "\n",
    "        self.ideal_flag_interval = ideal_flag_interval\n",
    "\n",
    "        self.autosave = None\n",
    "\n",
    "        self.model = self._make_model()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _make_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(146, 144, 2)))\n",
    "        model.add(layers.Conv2D(8, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units=256,activation='relu'))\n",
    "        model.add(layers.Dense(units=3,activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def preprocessFrame(self, I):\n",
    "        \"\"\" \n",
    "        Outputs a 72x72 image where background is black\n",
    "        and important game elements are white.\n",
    "        Output is [0,1]\n",
    "        \"\"\"\n",
    "        I = I[57:203, 8:152, 1]\n",
    "        return I/255\n",
    "\n",
    "    def decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_reward = 0\n",
    "        self.rewards = []\n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        self.lr_counter = 0\n",
    "        self.missing_flags = 20\n",
    "        self.last_frame = np.zeros((146, 144))\n",
    "\n",
    "    def action(self, frame, training=False):\n",
    "        frame = self.preprocessFrame(frame)\n",
    "        data = np.zeros((146, 144, 2))\n",
    "        data[:,:,0] = self.last_frame\n",
    "        data[:,:,0] = frame\n",
    "        self.last_frame = frame\n",
    "        probs = self.model.predict(np.expand_dims(data, 0))\n",
    "        #y = np.random.choice([0, 1, 2], p=probs[0])\n",
    "        y = np.argmax(probs[0])\n",
    "        \n",
    "        if float('nan') in probs[0]:\n",
    "            print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "            exit()\n",
    "\n",
    "        # Append flattened frame to x_train\n",
    "        self.train_x.append(data)\n",
    "        # Append selected action to y_train\n",
    "        self.train_y.append(to_categorical(y, num_classes=3))\n",
    "        return y\n",
    "\n",
    "    def register_frame(self, frame, rew = 0):\n",
    "        frame_reward = get_frame_reward(frame, self.last_reward)\n",
    "        reward = rew\n",
    "        self.lr_counter += 1\n",
    "        if frame_reward == 0 and self.last_reward != 0:\n",
    "            reward += self.last_reward + 2\n",
    "            reward -= 0.25 * np.tanh((0.05 * (self.lr_counter - self.ideal_flag_interval)))\n",
    "            self.lr_counter = 0\n",
    "            self.missing_flags -= 1\n",
    "        self.last_reward = frame_reward\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def done(self):\n",
    "        self.rewards[-1] -= self.missing_flags * 5\n",
    "\n",
    "    def train(self, verbose=0):\n",
    "        if self.autosave is not None and self.episode % self.autosave == 0:\n",
    "            self.save(\"last_lf.h5\")\n",
    "            print(\"Saved!\")\n",
    "        \n",
    "        #print(\"missed:\", self.missing_flags, \"flags\")\n",
    "        #self.rewards[-1] -= self.missing_flags * 5\n",
    "        sample_weights = discount_rewards(self.rewards, self.gamma)\n",
    "        self.model.fit(\n",
    "            x=np.array(self.train_x),\n",
    "            y=np.vstack(self.train_y),\n",
    "            verbose=verbose,\n",
    "            sample_weight=sample_weights\n",
    "        )\n",
    "        self.episode += 1\n",
    "        self.decay()\n",
    "\n",
    "    def total_reward(self):\n",
    "        return np.array(self.rewards).sum()\n",
    "\n",
    "    def set_autosave(self, interval):\n",
    "        self.autosave = interval\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jpins\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 144, 142, 8)       152       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 142, 140, 8)       584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 71, 70, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 69, 68, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 67, 66, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 33, 33, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 17424)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4460800   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 4,465,795\n",
      "Trainable params: 4,465,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Skier(gamma=0.99, e_decay=0.995)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# - = - = - = - #\n",
      "Ep:    0\n",
      "Total reward: 24.162\n",
      "Epsilon: 0.0500\n",
      "Saved!\n",
      "WARNING:tensorflow:From C:\\Users\\jpins\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "782/782 [==============================] - 3s 4ms/step - loss: -0.8364 - acc: 0.8721\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    1\n",
      "Total reward: 40.514\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "667/667 [==============================] - 2s 3ms/step - loss: 0.9114 - acc: 0.8906\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    2\n",
      "Total reward: 29.323\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "944/944 [==============================] - 2s 2ms/step - loss: -0.3230 - acc: 0.8390\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    3\n",
      "Total reward: 46.369\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 2s 2ms/step - loss: -0.1397 - acc: 0.8650\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    4\n",
      "Total reward: 30.035\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "871/871 [==============================] - 2s 2ms/step - loss: -0.7441 - acc: 0.8370\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    5\n",
      "Total reward: 34.004\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 2s 2ms/step - loss: -0.1941 - acc: 0.8716\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    6\n",
      "Total reward: 30.865\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 2s 2ms/step - loss: -0.3798 - acc: 0.8790\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    7\n",
      "Total reward: 28.995\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "1038/1038 [==============================] - 2s 2ms/step - loss: -0.0083 - acc: 0.7977\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    8\n",
      "Total reward: 42.343\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "757/757 [==============================] - 2s 3ms/step - loss: -0.6155 - acc: 0.8428\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:    9\n",
      "Total reward: 32.096\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 2s 2ms/step - loss: -0.9870 - acc: 0.8623\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   10\n",
      "Total reward: 22.646\n",
      "Epsilon: 0.0500\n",
      "Saved!\n",
      "Epoch 1/1\n",
      "1099/1099 [==============================] - 3s 2ms/step - loss: -0.4302 - acc: 0.8016\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   11\n",
      "Total reward: 23.050\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 2s 2ms/step - loss: -0.9382 - acc: 0.8941\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   12\n",
      "Total reward: 43.217\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "599/599 [==============================] - 2s 3ms/step - loss: -0.0782 - acc: 0.9048\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   13\n",
      "Total reward: 32.983\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "634/634 [==============================] - 2s 3ms/step - loss: -0.4695 - acc: 0.9069\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   14\n",
      "Total reward: 54.900\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "623/623 [==============================] - 2s 3ms/step - loss: 0.1842 - acc: 0.8892\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   15\n",
      "Total reward: 32.217\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "759/759 [==============================] - 2s 2ms/step - loss: -0.6172 - acc: 0.8854\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   16\n",
      "Total reward: 32.095\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "816/816 [==============================] - 2s 2ms/step - loss: 0.5444 - acc: 0.8358\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   17\n",
      "Total reward: 26.396\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "787/787 [==============================] - 2s 3ms/step - loss: -1.0550 - acc: 0.8767\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   18\n",
      "Total reward: 29.773\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "869/869 [==============================] - 2s 2ms/step - loss: -0.7275 - acc: 0.8285\n",
      "\n",
      "# - = - = - = - #\n",
      "Ep:   19\n",
      "Total reward: 38.771\n",
      "Epsilon: 0.0500\n",
      "Epoch 1/1\n",
      "640/664 [===========================>..] - ETA: 0s - loss: -0.9317 - acc: 0.8812"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-11db07a00f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m             print(\n\u001b[0;32m     94\u001b[0m                 f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7470bfc5ea12>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         )\n\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "agent.load(\"last_lf_best.h5\")\n",
    "\n",
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "agent.reset()\n",
    "\n",
    "observation_old = observation\n",
    "\n",
    "#other code\n",
    "\n",
    "cnt = 0\n",
    "r_a, c_a = get_pos_player(observation)\n",
    "r_f, c_f = get_pos_flags(observation)\n",
    "r_a_old, c_a_old = r_a, c_a\n",
    "#------------\n",
    "\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    #action = agent.action(observation, training=True)\n",
    "    \n",
    "\n",
    "    #other code\n",
    "    # TEACHER\n",
    "    v_f = np.arctan2(r_f - r_a, c_f - c_a) # direction from player to target\n",
    "    spd = get_speed(observation, observation_old)\n",
    "    v_a = np.arctan2(spd, c_a - c_a_old) # speed vector of the player\n",
    "    r_a_old, c_a_old = r_a, c_a\n",
    "    \n",
    "    \n",
    "    \n",
    "    frame = agent.preprocessFrame(observation)\n",
    "    data = np.zeros((146, 144, 2))\n",
    "    data[:,:,0] = agent.last_frame\n",
    "    data[:,:,1] = frame\n",
    "    agent.last_frame = frame\n",
    "        \n",
    "    \n",
    "    observation_old = observation\n",
    "    if np.random.rand() <= agent.epsilon:\n",
    "        if spd == 0 and (c_a - c_a_old) == 0:\n",
    "            cnt += 1\n",
    "            act_t = np.random.choice(3, 1)[0]\n",
    "        else:\n",
    "            cnt = 0\n",
    "            if v_f - v_a < -0.1:\n",
    "                act_t = 1\n",
    "            elif v_f - v_a > 0.1:\n",
    "                act_t = 2\n",
    "            else:\n",
    "                act_t = 0\n",
    "\n",
    "    else:\n",
    "        if spd == 0 and (c_a - c_a_old) == 0:\n",
    "            cnt += 1\n",
    "            act_t = np.random.choice(3, 1)[0]\n",
    "        else:\n",
    "            cnt = 0\n",
    "            probs = agent.model.predict(np.expand_dims(data, 0))\n",
    "            act_t = np.argmax(probs[0])\n",
    "            if float('nan') == probs[0][0]:\n",
    "                print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "                exit()\n",
    "\n",
    "\n",
    "    \n",
    "    # Append flattened frame to x_train\n",
    "    agent.train_x.append(data)\n",
    "    # Append selected action to y_train\n",
    "    agent.train_y.append(to_categorical(act_t, num_classes=3))\n",
    "    \n",
    "    observation, reward, done, info = env.step(act_t)\n",
    "    r_a, c_a = get_pos_player(observation)\n",
    "    r_f, c_f = get_pos_flags(observation)\n",
    "    #------------\n",
    "\n",
    "    \n",
    "    \n",
    "    if cnt > 30:\n",
    "        done = True\n",
    "        agent.register_frame(observation,-100.0)\n",
    "        cnt = 0\n",
    "    else:\n",
    "        agent.register_frame(observation)\n",
    "    \n",
    "    if done:\n",
    "        #agent.done()\n",
    "        total_reward = agent.total_reward()\n",
    "\n",
    "        if agent.episode % 1 == 0:\n",
    "            print('\\n# - = - = - = - #')\n",
    "            print(\n",
    "                f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n",
    "        agent.train(verbose=1)\n",
    "        agent.reset()\n",
    "\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"last_lf_best.h5\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load(\"last_lf_best.h5\")\n",
    "\n",
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "agent.reset()\n",
    "\n",
    "observation_old = observation\n",
    "\n",
    "#other code\n",
    "\n",
    "cnt = 0\n",
    "r_a, c_a = get_pos_player(observation)\n",
    "r_f, c_f = get_pos_flags(observation)\n",
    "r_a_old, c_a_old = r_a, c_a\n",
    "#------------\n",
    "\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    #action = agent.action(observation, training=True)\n",
    "    \n",
    "\n",
    "    #other code\n",
    "    # TEACHER\n",
    "    v_f = np.arctan2(r_f - r_a, c_f - c_a) # direction from player to target\n",
    "    spd = get_speed(observation, observation_old)\n",
    "    v_a = np.arctan2(spd, c_a - c_a_old) # speed vector of the player\n",
    "    r_a_old, c_a_old = r_a, c_a\n",
    "    \n",
    "    \n",
    "    \n",
    "    frame = agent.preprocessFrame(observation)\n",
    "    data = np.zeros((146, 144, 2))\n",
    "    data[:,:,0] = agent.last_frame\n",
    "    data[:,:,0] = frame\n",
    "    agent.last_frame = frame\n",
    "        \n",
    "    \n",
    "    observation_old = observation\n",
    "\n",
    "    if spd == 0 and (c_a - c_a_old) == 0:\n",
    "        cnt += 1\n",
    "        act_t = np.random.choice(3, 1)[0]\n",
    "    else:\n",
    "        cnt = 0\n",
    "        probs = agent.model.predict(np.expand_dims(data, 0))\n",
    "        act_t = np.argmax(probs[0])\n",
    "\n",
    "        if float('nan') in probs[0]:\n",
    "            print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "            exit()\n",
    "\n",
    "    observation, reward, done, info = env.step(act_t)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
