{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:NOOP 1:RIGHT 2:LEFT\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "action_size = env.action_space.n\n",
    "print(\" \".join( [f\"{i}:{a}\" for i,a in enumerate(env.get_action_meanings())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] is not 0: running_add = 0 # Reset running add for each game \"stage\"\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    #discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    #discounted_r /= np.std(discounted_r) #idem\n",
    "    return discounted_r\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, gamma=0.95, epsilon=1, e_min=0.05, e_decay=0.99):\n",
    "        self.episode = 0\n",
    "        self.frame = 0\n",
    "        self.model = self._make_model()\n",
    "        self.restart()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.autosave = None\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = e_min\n",
    "        self.epsilon_decay = e_decay\n",
    "        \n",
    "    def decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def preprocessFrame(self,I):\n",
    "        \"\"\" \n",
    "        Outputs a 72x72 image where background is black\n",
    "        and important game elements are white.\n",
    "        Output is [0,1]\n",
    "        \"\"\"\n",
    "        I = I[::2,::2,1]\n",
    "        I = I[31:103,4:76]\n",
    "        I[I == 236] = 0\n",
    "        I[I == 192] = 0\n",
    "        I[I == 214] = 0\n",
    "        I[I != 0] = 255\n",
    "        return I/255\n",
    "    \n",
    "    def cutoutScore(self,I):\n",
    "        I = I[:,:,1]\n",
    "        I = I[30:40,65:83]\n",
    "        I[I == 236] = 255\n",
    "        return I\n",
    "    \n",
    "    def _make_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(\n",
    "            units=256,\n",
    "            input_dim=72*72,\n",
    "            activation='relu',\n",
    "            #kernel_initializer='glorot_uniform'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=128,\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=3,\n",
    "            activation='softmax',\n",
    "            #kernel_initializer='RandomNormal'\n",
    "        ))\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def restart(self):\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.rewards = []\n",
    "        self.last = np.zeros(72*72)\n",
    "        self.last_score_window = None\n",
    "        self.final_reward = 0\n",
    "        self.frame_counter = 0\n",
    "        \n",
    "    def record(self, obs):\n",
    "        A = self.cutoutScore(obs)\n",
    "        if self.last_score_window is not None and not np.array_equal(A, self.last_score_window):\n",
    "            self.rewards.append(10)\n",
    "        else:\n",
    "            self.rewards.append(0)\n",
    "        self.last_score_window = A\n",
    "        \n",
    "    def done(self, reward):\n",
    "        time_r = 4507 / self.frame_counter - 1\n",
    "        if time_r is 0: time_r = -100\n",
    "        #flag_r = 5*(20 - (-reward) // 500) if time_r > 0 else 0\n",
    "        #self.final_reward = time_r #+ flag_r\n",
    "        self.rewards[-1] = time_r\n",
    "        \n",
    "    def total_reward(self):\n",
    "        return np.array(self.rewards).sum()\n",
    "    \n",
    "    def action(self, frame, training=False):\n",
    "        self.frame_counter  += 1\n",
    "        frame = self.preprocessFrame(frame).flatten()\n",
    "        #x = np.array([frame - self.last])\n",
    "        x = np.array([frame])\n",
    "        probs = self.model.predict(x)\n",
    "        y = np.random.choice([0,1,2], p=probs[0])\n",
    "        \n",
    "        if not training:\n",
    "            return y\n",
    "        else:\n",
    "            # Explore a bit\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                y = np.random.choice([0,1,2])\n",
    "                \n",
    "        # Append flattened frame to x_train \n",
    "        self.x_train.append(frame)\n",
    "        # Append selected action to y_train\n",
    "        self.y_train.append(to_categorical(y, num_classes=3))\n",
    "        # Append a 0 to sample weight \n",
    "        # Will be updated later\n",
    "        #self.rewards.append(0)\n",
    "        self.last = frame\n",
    "        return y\n",
    "        \n",
    "    def train(self, verbose=0):\n",
    "        #self.rewards[-1] = self.final_reward\n",
    "        #print(np.vstack(self.x_train).shape)\n",
    "        #print(discount_rewards(self.rewards, self.gamma))\n",
    "        #print(len(self.rewards))\n",
    "        self.model.fit(\n",
    "            x=np.vstack(self.x_train),\n",
    "            y=np.vstack(self.y_train),\n",
    "            verbose=verbose,\n",
    "            sample_weight=discount_rewards(self.rewards, self.gamma)\n",
    "        )\n",
    "        if self.autosave is not None and self.episode % self.autosave == 0:\n",
    "            self.save(\"last.h5\")\n",
    "            print(\"Saved!\")\n",
    "        self.episode += 1\n",
    "        self.decay()\n",
    "        \n",
    "    def set_autosave(self, interval):\n",
    "        self.autosave = interval\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 256)               1327360   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,360,643\n",
      "Trainable params: 1,360,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(gamma=0.95, e_decay=0.95)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardHist:\n",
    "    def __init__(self, maxlen=100):\n",
    "        self.mem = deque(maxlen=maxlen)\n",
    "        self.last = 0\n",
    "    \n",
    "    def add(self, reward):\n",
    "        self.mem.append(reward)\n",
    "    \n",
    "    def _nparr(self):\n",
    "        return np.array(self.mem)\n",
    "    \n",
    "    def max(self):\n",
    "        return self._nparr().max()\n",
    "    \n",
    "    def mean(self):\n",
    "        return self._nparr().mean()\n",
    "    \n",
    "    def report(self):\n",
    "        mean = self.mean()\n",
    "        symbol = '▲' if mean > self.last else '▼' if mean < self.last else '-'\n",
    "        print(f\"Reward AVG: {mean:8.2f} | {symbol} {(mean - self.last):8.2f}\")\n",
    "        print(f\"Best: {self.max()}\")\n",
    "        self.last = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# - = - = - = - #\n",
      "Ep:    0\n",
      "Total reward: 32.535\n",
      "Epsilon: 1.0000\n",
      "Reward AVG:    32.53 | ▲    32.53\n",
      "Best: 32.53490196078431\n",
      "Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\ipykernel_launcher.py:110: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# - = - = - = - #\n",
      "Ep:    1\n",
      "Total reward: 13.154\n",
      "Epsilon: 0.9500\n",
      "Reward AVG:    22.84 | ▼    -9.69\n",
      "Best: 32.53490196078431\n",
      "# - = - = - = - #\n",
      "Ep:    2\n",
      "Total reward: 44.627\n",
      "Epsilon: 0.9025\n",
      "Reward AVG:    30.11 | ▲     7.26\n",
      "Best: 44.62671660424469\n",
      "# - = - = - = - #\n",
      "Ep:    3\n",
      "Total reward: 62.224\n",
      "Epsilon: 0.8574\n",
      "Reward AVG:    38.13 | ▲     8.03\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:    4\n",
      "Total reward: 42.591\n",
      "Epsilon: 0.8145\n",
      "Reward AVG:    39.03 | ▲     0.89\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:    5\n",
      "Total reward: 52.688\n",
      "Epsilon: 0.7738\n",
      "Reward AVG:    41.30 | ▲     2.28\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:    6\n",
      "Total reward: 32.670\n",
      "Epsilon: 0.7351\n",
      "Reward AVG:    40.07 | ▼    -1.23\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:    7\n",
      "Total reward: 61.959\n",
      "Epsilon: 0.6983\n",
      "Reward AVG:    42.81 | ▲     2.74\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:    8\n",
      "Total reward: 42.516\n",
      "Epsilon: 0.6634\n",
      "Reward AVG:    42.77 | ▼    -0.03\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:    9\n",
      "Total reward: 12.594\n",
      "Epsilon: 0.6302\n",
      "Reward AVG:    39.76 | ▼    -3.02\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:   10\n",
      "Total reward: 32.233\n",
      "Epsilon: 0.5987\n",
      "Reward AVG:    39.07 | ▼    -0.68\n",
      "Best: 62.223891273247496\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   11\n",
      "Total reward: 52.417\n",
      "Epsilon: 0.5688\n",
      "Reward AVG:    40.18 | ▲     1.11\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:   12\n",
      "Total reward: 32.740\n",
      "Epsilon: 0.5404\n",
      "Reward AVG:    39.61 | ▼    -0.57\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:   13\n",
      "Total reward: 13.260\n",
      "Epsilon: 0.5133\n",
      "Reward AVG:    37.73 | ▼    -1.88\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:   14\n",
      "Total reward: 22.875\n",
      "Epsilon: 0.4877\n",
      "Reward AVG:    36.74 | ▼    -0.99\n",
      "Best: 62.223891273247496\n",
      "# - = - = - = - #\n",
      "Ep:   15\n",
      "Total reward: 54.235\n",
      "Epsilon: 0.4633\n",
      "Reward AVG:    37.83 | ▲     1.09\n",
      "Best: 62.223891273247496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f069796b2dac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    352\u001b[0m         gl.glTexParameteri(gl.GL_TEXTURE_2D, \n\u001b[0;32m    353\u001b[0m             gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mtexture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mget_texture\u001b[1;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    857\u001b[0m             self._current_texture = self.create_texture(Texture,\n\u001b[0;32m    858\u001b[0m                                                         \u001b[0mrectangle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                                                         force_rectangle)\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mcreate_texture\u001b[1;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         texture = cls.create(self.width, self.height, internalformat,\n\u001b[1;32m--> 844\u001b[1;33m                              rectangle, force_rectangle)\n\u001b[0m\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                      \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1563\u001b[0m                      \u001b[0mGL_RGBA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGL_UNSIGNED_BYTE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                      blank)\n\u001b[0m\u001b[0;32m   1565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m         \u001b[0mtexture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexture_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexture_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franspaco\\documents\\gitprojects\\inconspicuous\\venv\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "hist = RewardHist(100)\n",
    "agent.restart()\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    action = agent.action(observation, training=True)\n",
    "    \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    agent.record(observation)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        agent.done(reward)\n",
    "        total_reward = agent.total_reward()\n",
    "        hist.add(total_reward)\n",
    "        if agent.episode % 1 == 0:\n",
    "            print('# - = - = - = - #')\n",
    "            print(f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n",
    "            hist.report()\n",
    "        agent.train()\n",
    "        agent.restart()\n",
    "        \n",
    "        observation = env.reset()\n",
    "        #break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward: 12.33851851851852\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "agent.restart()\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    action = agent.action(observation)\n",
    "        \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        agent.done(reward)\n",
    "        print(f\"Final reward: {agent.final_reward}\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
