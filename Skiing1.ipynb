{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:NOOP 1:RIGHT 2:LEFT\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "action_size = env.action_space.n\n",
    "print(\" \".join( [f\"{i}:{a}\" for i,a in enumerate(env.get_action_meanings())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    discounted_r /= np.std(discounted_r) #idem\n",
    "    return discounted_r\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, gamma=0.95):\n",
    "        self.episode = 0\n",
    "        self.model = self._make_model()\n",
    "        self.restart()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.add_total = True\n",
    "        \n",
    "        self.autosave = None\n",
    "    \n",
    "    def preprocessFrame(self,I):\n",
    "        \"\"\" \n",
    "        Outputs a 72x72 image where background is black\n",
    "        and important game elements are white.\n",
    "        Output is [0,1]\n",
    "        \"\"\"\n",
    "        I = I[::2,::2,1]\n",
    "        I = I[31:103,4:76]\n",
    "        I[I == 236] = 0\n",
    "        I[I == 192] = 0\n",
    "        I[I == 214] = 0\n",
    "        I[I != 0] = 255\n",
    "        return I/255\n",
    "    \n",
    "    def _make_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(\n",
    "            units=256,\n",
    "            input_dim=72*72,\n",
    "            activation='relu',\n",
    "            #kernel_initializer='glorot_uniform'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=3,\n",
    "            activation='softmax',\n",
    "            #kernel_initializer='RandomNormal'\n",
    "        ))\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def restart(self):\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.rewards = []\n",
    "        self.last = np.zeros(72*72)\n",
    "        self.total_reward = 0\n",
    "        \n",
    "    def save_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "        self.total_reward += reward\n",
    "    \n",
    "    def action(self, frame):\n",
    "        frame = self.preprocessFrame(frame).flatten()\n",
    "        #x = np.array([frame - self.last])\n",
    "        x = np.array([frame])\n",
    "        probs = self.model.predict(x)\n",
    "        y = np.random.choice([0,1,2], p=probs[0])\n",
    "        self.x_train.append(x)\n",
    "        self.y_train.append(to_categorical(y, num_classes=3))\n",
    "        self.last = frame\n",
    "        return y\n",
    "        \n",
    "    def train(self):\n",
    "        self.episode += 1\n",
    "        if self.add_total: self.rewards[-1] += self.total_reward\n",
    "        self.model.fit(\n",
    "            x=np.vstack(self.x_train),\n",
    "            y=np.vstack(self.y_train),\n",
    "            verbose=1,\n",
    "            sample_weight=discount_rewards(self.rewards, self.gamma)\n",
    "        )\n",
    "        if self.autosave is not None and self.episode % self.autosave == 0:\n",
    "            self.model.save(\"last.h5\")\n",
    "            print(\"Saved!\")\n",
    "        \n",
    "    def set_autosave(self, interval):\n",
    "        self.autosave = interval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               1327360   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 1,328,131\n",
      "Trainable params: 1,328,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:    0\n",
      "Total reward: -13789.0\n",
      "Epoch 1/1\n",
      "1096/1096 [==============================] - 0s 319us/step - loss: -0.0023 - acc: 0.3741\n",
      "Ep:    1\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 130us/step - loss: 0.0052 - acc: 0.8633\n",
      "Ep:    2\n",
      "Total reward: -23312.0\n",
      "Epoch 1/1\n",
      "2226/2226 [==============================] - 0s 126us/step - loss: 0.0157 - acc: 0.5004\n",
      "Ep:    3\n",
      "Total reward: -15089.0\n",
      "Epoch 1/1\n",
      "915/915 [==============================] - 0s 124us/step - loss: 0.0241 - acc: 0.4929\n",
      "Ep:    4\n",
      "Total reward: -18008.0\n",
      "Epoch 1/1\n",
      "1429/1429 [==============================] - 0s 121us/step - loss: 0.0025 - acc: 0.5059\n",
      "Ep:    5\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 137us/step - loss: 0.0084 - acc: 0.7113\n",
      "Ep:    6\n",
      "Total reward: -15175.0\n",
      "Epoch 1/1\n",
      "1003/1003 [==============================] - 0s 125us/step - loss: 0.0097 - acc: 0.4606\n",
      "Ep:    7\n",
      "Total reward: -20114.0\n",
      "Epoch 1/1\n",
      "1745/1745 [==============================] - 0s 123us/step - loss: -0.0234 - acc: 0.4997\n",
      "Ep:    8\n",
      "Total reward: -16541.0\n",
      "Epoch 1/1\n",
      "1209/1209 [==============================] - 0s 121us/step - loss: 0.0271 - acc: 0.4864\n",
      "Ep:    9\n",
      "Total reward: -16215.0\n",
      "Epoch 1/1\n",
      "1009/1009 [==============================] - 0s 121us/step - loss: 0.0279 - acc: 0.5084\n",
      "Saved!\n",
      "Ep:   10\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 128us/step - loss: 0.0066 - acc: 0.7577\n",
      "Ep:   11\n",
      "Total reward: -18975.0\n",
      "Epoch 1/1\n",
      "1950/1950 [==============================] - 0s 123us/step - loss: 0.0171 - acc: 0.5667\n",
      "Ep:   12\n",
      "Total reward: -23027.0\n",
      "Epoch 1/1\n",
      "2258/2258 [==============================] - 0s 122us/step - loss: 0.0118 - acc: 0.5372\n",
      "Ep:   13\n",
      "Total reward: -22248.0\n",
      "Epoch 1/1\n",
      "1991/1991 [==============================] - 0s 124us/step - loss: 0.0175 - acc: 0.5153\n",
      "Ep:   14\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 131us/step - loss: 0.0090 - acc: 0.6818\n",
      "Ep:   15\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 126us/step - loss: 0.0054 - acc: 0.8218\n",
      "Ep:   16\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 131us/step - loss: 0.0065 - acc: 0.8476\n",
      "Ep:   17\n",
      "Total reward: -13963.0\n",
      "Epoch 1/1\n",
      "972/972 [==============================] - 0s 125us/step - loss: 0.0113 - acc: 0.5103\n",
      "Ep:   18\n",
      "Total reward: -15191.0\n",
      "Epoch 1/1\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.0282 - acc: 0.4801\n",
      "Ep:   19\n",
      "Total reward: -18731.0\n",
      "Epoch 1/1\n",
      "1387/1387 [==============================] - 0s 132us/step - loss: 0.0204 - acc: 0.5155\n",
      "Saved!\n",
      "Ep:   20\n",
      "Total reward: -25713.0\n",
      "Epoch 1/1\n",
      "2587/2587 [==============================] - 0s 134us/step - loss: 0.0093 - acc: 0.5864\n",
      "Ep:   21\n",
      "Total reward: -37885.0\n",
      "Epoch 1/1\n",
      "4490/4490 [==============================] - 1s 130us/step - loss: -2.2450e-04 - acc: 0.6880\n",
      "Ep:   22\n",
      "Total reward: -26985.0\n",
      "Epoch 1/1\n",
      "2703/2703 [==============================] - 0s 125us/step - loss: 0.0169 - acc: 0.5753\n",
      "Ep:   23\n",
      "Total reward: -30746.0\n",
      "Epoch 1/1\n",
      "3268/3268 [==============================] - 0s 130us/step - loss: -0.0281 - acc: 0.6958\n",
      "Ep:   24\n",
      "Total reward: -15451.0\n",
      "Epoch 1/1\n",
      "1045/1045 [==============================] - 0s 124us/step - loss: 0.0296 - acc: 0.5349\n",
      "Ep:   25\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 130us/step - loss: 0.0063 - acc: 0.7759\n",
      "Ep:   26\n",
      "Total reward: -21186.0\n",
      "Epoch 1/1\n",
      "1831/1831 [==============================] - 0s 126us/step - loss: 0.0211 - acc: 0.5593\n",
      "Ep:   27\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 143us/step - loss: 0.0076 - acc: 0.7710\n",
      "Ep:   28\n",
      "Total reward: -23524.0\n",
      "Epoch 1/1\n",
      "2333/2333 [==============================] - 0s 121us/step - loss: 0.0165 - acc: 0.6331\n",
      "Ep:   29\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 147us/step - loss: 0.0071 - acc: 0.8422\n",
      "Saved!\n",
      "Ep:   30\n",
      "Total reward: -34236.0\n",
      "Epoch 1/1\n",
      "3867/3867 [==============================] - 0s 128us/step - loss: 0.0104 - acc: 0.7210\n",
      "Ep:   31\n",
      "Total reward: -23048.0\n",
      "Epoch 1/1\n",
      "2261/2261 [==============================] - 0s 123us/step - loss: 0.0166 - acc: 0.6099\n",
      "Ep:   32\n",
      "Total reward: -30530.0\n",
      "Epoch 1/1\n",
      "3385/3385 [==============================] - 0s 130us/step - loss: 0.0058 - acc: 0.5968\n",
      "Ep:   33\n",
      "Total reward: -30000.0\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 128us/step - loss: 0.0071 - acc: 0.8389\n",
      "Ep:   34\n",
      "Total reward: -19633.0\n",
      "Epoch 1/1\n",
      "1748/1748 [==============================] - 0s 125us/step - loss: 0.0194 - acc: 0.6018\n",
      "Ep:   35\n",
      "Total reward: -37946.0\n",
      "Epoch 1/1\n",
      "4424/4424 [==============================] - 1s 150us/step - loss: 0.0120 - acc: 0.6388\n",
      "Ep:   36\n",
      "Total reward: -16720.0\n",
      "Epoch 1/1\n",
      "1236/1236 [==============================] - 0s 135us/step - loss: 0.0269 - acc: 0.5461\n",
      "Ep:   37\n",
      "Total reward: -29874.0\n",
      "Epoch 1/1\n",
      "3437/3437 [==============================] - 0s 131us/step - loss: -0.0541 - acc: 0.73760s - loss: 0.0104 - \n"
     ]
    }
   ],
   "source": [
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "agent.restart()\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    action = agent.action(observation)\n",
    "    \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    agent.save_reward(reward)\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Ep: {agent.episode:4}\\nTotal reward: {agent.total_reward}\")\n",
    "        agent.train()\n",
    "        agent.restart()\n",
    "        \n",
    "        observation = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "agent.restart()\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    action = agent.action(observation)\n",
    "        \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    agent.save_reward(reward)\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Total reward: {agent.total_reward}\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
