{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:NOOP 1:RIGHT 2:LEFT\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "action_size = env.action_space.n\n",
    "print(\" \".join( [f\"{i}:{a}\" for i,a in enumerate(env.get_action_meanings())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    #discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    #discounted_r /= np.std(discounted_r) #idem\n",
    "    return discounted_r\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, gamma=0.95):\n",
    "        self.episode = 0\n",
    "        self.frame = 0\n",
    "        self.model = self._make_model()\n",
    "        self.restart()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.add_total = True\n",
    "        \n",
    "        self.autosave = None\n",
    "    \n",
    "    def preprocessFrame(self,I):\n",
    "        \"\"\" \n",
    "        Outputs a 72x72 image where background is black\n",
    "        and important game elements are white.\n",
    "        Output is [0,1]\n",
    "        \"\"\"\n",
    "        I = I[::2,::2,1]\n",
    "        I = I[31:103,4:76]\n",
    "        I[I == 236] = 0\n",
    "        I[I == 192] = 0\n",
    "        I[I == 214] = 0\n",
    "        I[I != 0] = 255\n",
    "        return I/255\n",
    "    \n",
    "    def _make_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(\n",
    "            units=256,\n",
    "            input_dim=72*72,\n",
    "            activation='relu',\n",
    "            #kernel_initializer='glorot_uniform'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=128,\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=3,\n",
    "            activation='softmax',\n",
    "            #kernel_initializer='RandomNormal'\n",
    "        ))\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def restart(self):\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.rewards = []\n",
    "        self.last = np.zeros(72*72)\n",
    "        self.final_reward = 0\n",
    "        self.frame_counter = 0\n",
    "        \n",
    "    def done(self, reward):\n",
    "        if done:\n",
    "            time_r = 4507 / self.frame_counter - 1\n",
    "            flag_r = 20 - (-reward) // 500 if time_r > 0 else -10\n",
    "            self.final_reward = time_r + flag_r\n",
    "    \n",
    "    def action(self, frame):\n",
    "        frame = self.preprocessFrame(frame).flatten()\n",
    "        #x = np.array([frame - self.last])\n",
    "        x = np.array([frame])\n",
    "        probs = self.model.predict(x)\n",
    "        y = np.random.choice([0,1,2], p=probs[0])\n",
    "        # Append flattened frame to x_train \n",
    "        self.x_train.append(frame)\n",
    "        # Append selected action to y_train\n",
    "        self.y_train.append(to_categorical(y, num_classes=3))\n",
    "        # Append a 0 to sample weight \n",
    "        # Will be updated later\n",
    "        self.rewards.append(0)\n",
    "        self.last = frame\n",
    "        self.frame_counter  += 1 \n",
    "        return y\n",
    "        \n",
    "    def train(self):\n",
    "        if self.add_total: self.rewards[-1] = self.final_reward\n",
    "        #print(np.vstack(self.x_train).shape)\n",
    "        #print(discount_rewards(self.rewards, self.gamma))\n",
    "        #print(len(self.rewards))\n",
    "        self.model.fit(\n",
    "            x=np.vstack(self.x_train),\n",
    "            y=np.vstack(self.y_train),\n",
    "            verbose=1,\n",
    "            sample_weight=discount_rewards(self.rewards, self.gamma)\n",
    "        )\n",
    "        if self.autosave is not None and self.episode % self.autosave == 0:\n",
    "            self.save(\"last.h5\")\n",
    "            print(\"Saved!\")\n",
    "        self.episode += 1\n",
    "        \n",
    "    def set_autosave(self, interval):\n",
    "        self.autosave = interval\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               1327360   \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,360,643\n",
      "Trainable params: 1,360,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(gamma=0.98)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardHist:\n",
    "    def __init__(self, maxlen=100):\n",
    "        self.mem = deque(maxlen=maxlen)\n",
    "        self.last = 0\n",
    "    \n",
    "    def add(self, reward):\n",
    "        self.mem.append(reward)\n",
    "    \n",
    "    def _nparr(self):\n",
    "        return np.array(self.mem)\n",
    "    \n",
    "    def max(self):\n",
    "        return self._nparr().max()\n",
    "    \n",
    "    def mean(self):\n",
    "        return self._nparr().mean()\n",
    "    \n",
    "    def report(self):\n",
    "        mean = self.mean()\n",
    "        symbol = '▲' if mean > self.last else '▼' if mean < self.last else '-'\n",
    "        print(f\"Reward AVG: {mean:8.2f} | {symbol} {(mean - self.last):8.2f}\")\n",
    "        print(f\"Best: {self.max()}\")\n",
    "        self.last = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# - = - = - = - #\n",
      "Ep:    0\n",
      "Total reward: 3.172\n",
      "Reward AVG:     3.17 | ▲     3.17\n",
      "Best: 3.171710063335679\n",
      "Epoch 1/1\n",
      "1421/1421 [==============================] - 1s 391us/step - loss: 0.1412 - acc: 0.3906\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:    1\n",
      "Total reward: 5.855\n",
      "Reward AVG:     4.51 | ▲     1.34\n",
      "Best: 5.855431993156544\n",
      "Epoch 1/1\n",
      "1169/1169 [==============================] - 0s 131us/step - loss: 0.3073 - acc: 0.3730\n",
      "# - = - = - = - #\n",
      "Ep:    2\n",
      "Total reward: 6.043\n",
      "Reward AVG:     5.02 | ▲     0.51\n",
      "Best: 6.0432140445644835\n",
      "Epoch 1/1\n",
      "1481/1481 [==============================] - 0s 132us/step - loss: 0.2337 - acc: 0.4132\n",
      "# - = - = - = - #\n",
      "Ep:    3\n",
      "Total reward: 8.154\n",
      "Reward AVG:     5.81 | ▲     0.78\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1085/1085 [==============================] - 0s 133us/step - loss: 0.4120 - acc: 0.4341\n",
      "# - = - = - = - #\n",
      "Ep:    4\n",
      "Total reward: 6.285\n",
      "Reward AVG:     5.90 | ▲     0.10\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1372/1372 [==============================] - 0s 138us/step - loss: 0.2129 - acc: 0.4169\n",
      "# - = - = - = - #\n",
      "Ep:    5\n",
      "Total reward: 1.805\n",
      "Reward AVG:     5.22 | ▼    -0.68\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1607/1607 [==============================] - 0s 140us/step - loss: 0.0468 - acc: 0.4599\n",
      "# - = - = - = - #\n",
      "Ep:    6\n",
      "Total reward: 3.222\n",
      "Reward AVG:     4.93 | ▼    -0.29\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1399/1399 [==============================] - 0s 135us/step - loss: 0.1057 - acc: 0.4024\n",
      "# - = - = - = - #\n",
      "Ep:    7\n",
      "Total reward: 5.950\n",
      "Reward AVG:     5.06 | ▲     0.13\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1528/1528 [==============================] - 0s 135us/step - loss: 0.1981 - acc: 0.4457\n",
      "# - = - = - = - #\n",
      "Ep:    8\n",
      "Total reward: 4.462\n",
      "Reward AVG:     4.99 | ▼    -0.07\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1302/1302 [==============================] - 0s 141us/step - loss: 0.1564 - acc: 0.4224\n",
      "# - = - = - = - #\n",
      "Ep:    9\n",
      "Total reward: 0.259\n",
      "Reward AVG:     4.52 | ▼    -0.47\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "3579/3579 [==============================] - 0s 137us/step - loss: 0.0019 - acc: 0.5166\n",
      "# - = - = - = - #\n",
      "Ep:   10\n",
      "Total reward: 3.642\n",
      "Reward AVG:     4.44 | ▼    -0.08\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1706/1706 [==============================] - 0s 140us/step - loss: 0.0866 - acc: 0.4607\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   11\n",
      "Total reward: 4.053\n",
      "Reward AVG:     4.41 | ▼    -0.03\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1112/1112 [==============================] - 0s 143us/step - loss: 0.1144 - acc: 0.4281\n",
      "# - = - = - = - #\n",
      "Ep:   12\n",
      "Total reward: 4.813\n",
      "Reward AVG:     4.44 | ▲     0.03\n",
      "Best: 8.153917050691245\n",
      "Epoch 1/1\n",
      "1182/1182 [==============================] - 0s 134us/step - loss: 0.1383 - acc: 0.4535\n",
      "# - = - = - = - #\n",
      "Ep:   13\n",
      "Total reward: 12.638\n",
      "Reward AVG:     5.03 | ▲     0.59\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1239/1239 [==============================] - 0s 134us/step - loss: 0.5049 - acc: 0.4334\n",
      "# - = - = - = - #\n",
      "Ep:   14\n",
      "Total reward: 10.003\n",
      "Reward AVG:     5.36 | ▲     0.33\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1126/1126 [==============================] - 0s 139us/step - loss: 0.3713 - acc: 0.4432\n",
      "# - = - = - = - #\n",
      "Ep:   15\n",
      "Total reward: 8.467\n",
      "Reward AVG:     5.55 | ▲     0.19\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1300/1300 [==============================] - 0s 137us/step - loss: 0.2822 - acc: 0.4577\n",
      "# - = - = - = - #\n",
      "Ep:   16\n",
      "Total reward: 3.475\n",
      "Reward AVG:     5.43 | ▼    -0.12\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1297/1297 [==============================] - 0s 139us/step - loss: 0.0889 - acc: 0.4449\n",
      "# - = - = - = - #\n",
      "Ep:   17\n",
      "Total reward: 4.574\n",
      "Reward AVG:     5.38 | ▼    -0.05\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1261/1261 [==============================] - 0s 136us/step - loss: 0.1156 - acc: 0.4417\n",
      "# - = - = - = - #\n",
      "Ep:   18\n",
      "Total reward: 3.123\n",
      "Reward AVG:     5.26 | ▼    -0.12\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1443/1443 [==============================] - 0s 141us/step - loss: 0.0803 - acc: 0.4900\n",
      "# - = - = - = - #\n",
      "Ep:   19\n",
      "Total reward: 10.778\n",
      "Reward AVG:     5.54 | ▲     0.28\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1193/1193 [==============================] - 0s 140us/step - loss: 0.2201 - acc: 0.4677\n",
      "# - = - = - = - #\n",
      "Ep:   20\n",
      "Total reward: 7.985\n",
      "Reward AVG:     5.66 | ▲     0.12\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "1510/1510 [==============================] - 0s 137us/step - loss: 0.2017 - acc: 0.5219\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   21\n",
      "Total reward: -10.000\n",
      "Reward AVG:     4.94 | ▼    -0.71\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 150us/step - loss: -0.0681 - acc: 0.7766\n",
      "# - = - = - = - #\n",
      "Ep:   22\n",
      "Total reward: -10.000\n",
      "Reward AVG:     4.29 | ▼    -0.65\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 139us/step - loss: -1.3660 - acc: 0.1012\n",
      "# - = - = - = - #\n",
      "Ep:   23\n",
      "Total reward: -10.000\n",
      "Reward AVG:     3.70 | ▼    -0.60\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 151us/step - loss: -0.8697 - acc: 0.5585\n",
      "# - = - = - = - #\n",
      "Ep:   24\n",
      "Total reward: -10.000\n",
      "Reward AVG:     3.15 | ▼    -0.55\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 144us/step - loss: -1.6675 - acc: 0.1311\n",
      "# - = - = - = - #\n",
      "Ep:   25\n",
      "Total reward: -10.000\n",
      "Reward AVG:     2.62 | ▼    -0.53\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 144us/step - loss: -0.7547 - acc: 0.8613\n",
      "# - = - = - = - #\n",
      "Ep:   26\n",
      "Total reward: 12.443\n",
      "Reward AVG:     2.89 | ▲     0.26\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "828/828 [==============================] - 0s 137us/step - loss: 0.3329 - acc: 0.8780\n",
      "# - = - = - = - #\n",
      "Ep:   27\n",
      "Total reward: -10.000\n",
      "Reward AVG:     2.25 | ▼    -0.64\n",
      "Best: 12.637610976594027\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 146us/step - loss: -1.6883 - acc: 0.2117\n",
      "# - = - = - = - #\n",
      "Ep:   28\n",
      "Total reward: 14.109\n",
      "Reward AVG:     2.48 | ▲     0.24\n",
      "Best: 14.108832807570977\n",
      "Epoch 1/1\n",
      "634/634 [==============================] - 0s 135us/step - loss: 0.0603 - acc: 0.9811\n",
      "# - = - = - = - #\n",
      "Ep:   29\n",
      "Total reward: 14.053\n",
      "Reward AVG:     2.79 | ▲     0.31\n",
      "Best: 14.108832807570977\n",
      "Epoch 1/1\n",
      "639/639 [==============================] - 0s 137us/step - loss: 0.0277 - acc: 0.9922\n",
      "# - = - = - = - #\n",
      "Ep:   30\n",
      "Total reward: 15.639\n",
      "Reward AVG:     3.35 | ▲     0.55\n",
      "Best: 15.638983050847457\n",
      "Epoch 1/1\n",
      "590/590 [==============================] - 0s 140us/step - loss: 0.0310 - acc: 0.9966\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   31\n",
      "Total reward: 16.520\n",
      "Reward AVG:     3.88 | ▲     0.53\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "529/529 [==============================] - 0s 140us/step - loss: 0.1315 - acc: 0.9905\n",
      "# - = - = - = - #\n",
      "Ep:   32\n",
      "Total reward: -10.000\n",
      "Reward AVG:     3.24 | ▼    -0.64\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 145us/step - loss: -0.0233 - acc: 0.9860\n",
      "# - = - = - = - #\n",
      "Ep:   33\n",
      "Total reward: -10.000\n",
      "Reward AVG:     2.66 | ▼    -0.58\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 172us/step - loss: -1.3225e-08 - acc: 0.9996\n",
      "# - = - = - = - #\n",
      "Ep:   34\n",
      "Total reward: 11.669\n",
      "Reward AVG:     3.12 | ▲     0.46\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "795/795 [==============================] - 0s 152us/step - loss: 0.0369 - acc: 0.9723\n",
      "# - = - = - = - #\n",
      "Ep:   35\n",
      "Total reward: -10.000\n",
      "Reward AVG:     2.57 | ▼    -0.55\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "4507/4507 [==============================] - 1s 154us/step - loss: -1.7271 - acc: 0.1728\n",
      "# - = - = - = - #\n",
      "Ep:   36\n",
      "Total reward: 16.352\n",
      "Reward AVG:     3.07 | ▲     0.49\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 150us/step - loss: 0.0012 - acc: 0.9984\n",
      "# - = - = - = - #\n",
      "Ep:   37\n",
      "Total reward: 15.135\n",
      "Reward AVG:     3.48 | ▲     0.41\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 0.0019 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   38\n",
      "Total reward: 15.135\n",
      "Reward AVG:     3.58 | ▲     0.10\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 151us/step - loss: 2.6423e-04 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   39\n",
      "Total reward: 16.352\n",
      "Reward AVG:     3.83 | ▲     0.25\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 148us/step - loss: 1.7000e-04 - acc: 0.9984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# - = - = - = - #\n",
      "Ep:   40\n",
      "Total reward: 15.135\n",
      "Reward AVG:     4.10 | ▲     0.27\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 1.1016e-04 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   41\n",
      "Total reward: 16.352\n",
      "Reward AVG:     4.61 | ▲     0.52\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 150us/step - loss: 3.7782e-05 - acc: 0.9984\n",
      "# - = - = - = - #\n",
      "Ep:   42\n",
      "Total reward: 15.135\n",
      "Reward AVG:     5.04 | ▲     0.42\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 155us/step - loss: 7.5231e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   43\n",
      "Total reward: 16.352\n",
      "Reward AVG:     5.57 | ▲     0.53\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 143us/step - loss: 2.7631e-05 - acc: 0.9984\n",
      "# - = - = - = - #\n",
      "Ep:   44\n",
      "Total reward: 15.135\n",
      "Reward AVG:     5.74 | ▲     0.17\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 5.1021e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   45\n",
      "Total reward: 15.135\n",
      "Reward AVG:     6.03 | ▲     0.29\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 3.9926e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   46\n",
      "Total reward: 9.140\n",
      "Reward AVG:     6.79 | ▲     0.77\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 148us/step - loss: 0.0074 - acc: 0.9959\n",
      "# - = - = - = - #\n",
      "Ep:   47\n",
      "Total reward: 15.135\n",
      "Reward AVG:     7.80 | ▲     1.01\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 4.9408e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   48\n",
      "Total reward: 9.756\n",
      "Reward AVG:     8.59 | ▲     0.79\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "783/783 [==============================] - 0s 153us/step - loss: 1.3718e-04 - acc: 0.9987\n",
      "# - = - = - = - #\n",
      "Ep:   49\n",
      "Total reward: 16.352\n",
      "Reward AVG:     9.64 | ▲     1.05\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 142us/step - loss: 1.5988e-05 - acc: 0.9984\n",
      "# - = - = - = - #\n",
      "Ep:   50\n",
      "Total reward: 15.135\n",
      "Reward AVG:    10.65 | ▲     1.01\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 3.7699e-05 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   51\n",
      "Total reward: 15.135\n",
      "Reward AVG:    10.75 | ▲     0.11\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 137us/step - loss: 2.9119e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   52\n",
      "Total reward: 15.135\n",
      "Reward AVG:    11.76 | ▲     1.01\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 2.4880e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   53\n",
      "Total reward: 15.135\n",
      "Reward AVG:    11.80 | ▲     0.04\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 1.7317e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   54\n",
      "Total reward: 15.135\n",
      "Reward AVG:    11.84 | ▲     0.04\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 1.3990e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   55\n",
      "Total reward: 15.135\n",
      "Reward AVG:    11.82 | ▼    -0.02\n",
      "Best: 16.51984877126654\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 1.1915e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   56\n",
      "Total reward: 15.135\n",
      "Reward AVG:    11.77 | ▼    -0.06\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.0477e-05 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   57\n",
      "Total reward: 15.135\n",
      "Reward AVG:    12.77 | ▲     1.01\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 8.5055e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   58\n",
      "Total reward: 15.135\n",
      "Reward AVG:    13.78 | ▲     1.01\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 7.7612e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   59\n",
      "Total reward: 15.135\n",
      "Reward AVG:    13.92 | ▲     0.14\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 6.7945e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   60\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.92 | ▲     1.01\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 153us/step - loss: 6.0334e-06 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   61\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.88 | ▼    -0.05\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 155us/step - loss: 5.5174e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   62\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.88 | ▼    -0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 151us/step - loss: 5.0742e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   63\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.88 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 4.6455e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   64\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.83 | ▼    -0.05\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 4.2521e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   65\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.83 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 4.0873e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   66\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.78 | ▼    -0.05\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 3.7033e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   67\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.78 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 3.5040e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   68\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.73 | ▼    -0.05\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 151us/step - loss: 3.2728e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   69\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.73 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 158us/step - loss: 3.0939e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   70\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.73 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 2.9594e-06 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   71\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.97 | ▲     0.24\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 2.8024e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   72\n",
      "Total reward: 15.135\n",
      "Reward AVG:    14.97 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 2.6670e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   73\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | ▲     0.22\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 135us/step - loss: 2.5516e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   74\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | ▼    -0.05\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 2.4184e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   75\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 2.3693e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   76\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 2.2413e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   77\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 2.1493e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   78\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 133us/step - loss: 2.0800e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   79\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 0s 149us/step - loss: 1.9961e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   80\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 157us/step - loss: 1.9357e-06 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   81\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 1.8640e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   82\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 1.8070e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   83\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 137us/step - loss: 1.7390e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   84\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 1.6908e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   85\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 1.6399e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   86\n",
      "Total reward: 16.352\n",
      "Reward AVG:    15.18 | ▲     0.05\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 138us/step - loss: 2.2114e-05 - acc: 0.9984\n",
      "# - = - = - = - #\n",
      "Ep:   87\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 2.0903e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   88\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 133us/step - loss: 2.0791e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   89\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 1.9957e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   90\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 1.9098e-06 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:   91\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.8225e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   92\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 1.7712e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   93\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.6886e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   94\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 1.6476e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   95\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 1.5661e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   96\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 157us/step - loss: 1.5140e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   97\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 1.4752e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   98\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.4127e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:   99\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 1.3749e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  100\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.3160e-06 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:  101\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 1.2407e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  102\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 133us/step - loss: 1.2022e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  103\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 1.1539e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  104\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.1246e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  105\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 1.1032e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  106\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 1.0754e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  107\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 135us/step - loss: 1.0467e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  108\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 1.0215e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  109\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 151us/step - loss: 1.0003e-06 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  110\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.18 | -     0.00\n",
      "Best: 16.35236541598695\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 151us/step - loss: 9.7756e-07 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:  111\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | ▼    -0.05\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 9.5882e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  112\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 133us/step - loss: 9.3864e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  113\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 9.2092e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  114\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 8.9891e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  115\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 149us/step - loss: 8.8443e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  116\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 8.6296e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  117\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 8.4982e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  118\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 146us/step - loss: 8.3149e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  119\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 0s 149us/step - loss: 8.2112e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  120\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 7.9366e-07 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:  121\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 7.7876e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  122\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 7.6781e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  123\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 7.5598e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  124\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 7.4294e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  125\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 153us/step - loss: 7.3256e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  126\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 7.1957e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  127\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 137us/step - loss: 7.1021e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  128\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 7.0039e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  129\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 6.8780e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  130\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 135us/step - loss: 6.8268e-07 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:  131\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 6.6903e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  132\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 6.6073e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  133\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 151us/step - loss: 6.5305e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  134\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 149us/step - loss: 6.4244e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  135\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 137us/step - loss: 6.3482e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  136\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 6.2633e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  137\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 6.1832e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  138\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 139us/step - loss: 6.0984e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  139\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 6.0114e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  140\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 133us/step - loss: 5.9265e-07 - acc: 1.0000\n",
      "Saved!\n",
      "# - = - = - = - #\n",
      "Ep:  141\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 5.8708e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  142\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 5.7978e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  143\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 140us/step - loss: 5.7508e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  144\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 142us/step - loss: 5.6549e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  145\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 5.5991e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  146\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 144us/step - loss: 5.5289e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  147\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 148us/step - loss: 5.4673e-07 - acc: 1.0000\n",
      "# - = - = - = - #\n",
      "Ep:  148\n",
      "Total reward: 15.135\n",
      "Reward AVG:    15.14 | -     0.00\n",
      "Best: 15.135379061371841\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 153us/step - loss: 5.4018e-07 - acc: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-ac8750f87a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gitprojects\\proydl\\venv\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gitprojects\\proydl\\venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"ale.lives\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gitprojects\\proydl\\venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gitprojects\\proydl\\venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gitprojects\\proydl\\venv\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[1;34m(self, screen_data)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "hist = RewardHist(25)\n",
    "agent.restart()\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    action = agent.action(observation)\n",
    "    \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        agent.done(reward)\n",
    "        print('# - = - = - = - #')\n",
    "        print(f\"Ep: {agent.episode:4}\\nTotal reward: {agent.final_reward:.3f}\")\n",
    "        hist.add(agent.final_reward)\n",
    "        hist.report()\n",
    "        agent.train()\n",
    "        agent.restart()\n",
    "        \n",
    "        observation = env.reset()\n",
    "        #break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "agent.restart()\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    action = agent.action(observation)\n",
    "        \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    agent.save_reward(reward)\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Total reward: {agent.total_reward}\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
