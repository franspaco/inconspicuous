{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from RunHist import RewardHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_reward(I, prev):\n",
    "    I = I[:, :, 1]\n",
    "    I = I[74:75, 8:152]  # Jugador 92, bandera roja 50, bandera azul 72\n",
    "    if 72 not in I and 50 not in I:\n",
    "        return 0\n",
    "    if 72 in I:\n",
    "        flags = np.where(I == 72)\n",
    "    else:\n",
    "        flags = np.where(I == 50)\n",
    "    player = np.where(I == 92)[1]\n",
    "    if len(player) == 0:\n",
    "        return 1\n",
    "    player = player.mean()\n",
    "    if len(flags[1]) == 2:\n",
    "        if player >= flags[1][0] and player <= flags[1][1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        #if r[t] != 0:\n",
    "            # if the game ended (in Pong), reset the reward sum\n",
    "            #running_add = 0\n",
    "        # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r)  # normalizing the result\n",
    "    discounted_r /= np.std(discounted_r)  # idem\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skier:\n",
    "    def __init__(self, gamma=0.95, epsilon=1, e_min=0.05, e_decay=0.99, ideal_flag_interval=25):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = e_min\n",
    "        self.epsilon_decay = e_decay\n",
    "\n",
    "        self.episode = 0\n",
    "\n",
    "        self.ideal_flag_interval = ideal_flag_interval\n",
    "\n",
    "        self.autosave = None\n",
    "\n",
    "        self.model = self._make_model()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _make_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(146, 144, 3)))\n",
    "        model.add(layers.Conv2D(8, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(\n",
    "            units=128,\n",
    "            #input_dim=72*72,\n",
    "            activation='relu',\n",
    "            # kernel_initializer='glorot_uniform'\n",
    "        ))\n",
    "        # model.add(layers.Dense(\n",
    "        #     units=64,\n",
    "        #     activation='relu'\n",
    "        # ))\n",
    "        model.add(layers.Dense(\n",
    "            units=3,\n",
    "            activation='softmax',\n",
    "            # kernel_initializer='RandomNormal'\n",
    "        ))\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def preprocessFrame(self, I):\n",
    "        \"\"\" \n",
    "        Outputs a 72x72 image where background is black\n",
    "        and important game elements are white.\n",
    "        Output is [0,1]\n",
    "        \"\"\"\n",
    "        #I = I[::2, ::2, 1]\n",
    "        I = I[57:203, 8:152]\n",
    "        #I[I == 236] = 0\n",
    "        #I[I == 192] = 0\n",
    "        #I[I == 214] = 0\n",
    "        #I[I != 0] = 255\n",
    "        return I/255\n",
    "\n",
    "    def decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_reward = 0\n",
    "        self.rewards = []\n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        self.lr_counter = 0\n",
    "        self.missing_flags = 20\n",
    "\n",
    "    def action(self, frame, training=False):\n",
    "        frame = self.preprocessFrame(frame)\n",
    "        x = np.array([frame])\n",
    "        probs = self.model.predict(x)\n",
    "        y = np.random.choice([0, 1, 2], p=probs[0])\n",
    "        print(probs[0])\n",
    "        if float('nan') in probs[0]:\n",
    "            print(\"NANANANANANANANANANANANANANANANANANA\", probs[0])\n",
    "            exit()\n",
    "        if not training:\n",
    "            return y\n",
    "        else:\n",
    "            # Explore a bit\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                y = np.random.choice([0, 1, 2])\n",
    "\n",
    "        # Append flattened frame to x_train\n",
    "        self.train_x.append(frame)\n",
    "        # Append selected action to y_train\n",
    "        self.train_y.append(to_categorical(y, num_classes=3))\n",
    "        return y\n",
    "\n",
    "    def register_frame(self, frame):\n",
    "        frame_reward = get_frame_reward(frame, self.last_reward)\n",
    "        reward = 0\n",
    "        self.lr_counter += 1\n",
    "        if frame_reward == 0 and self.last_reward != 0:\n",
    "            reward = self.last_reward\n",
    "            reward -= 0.5 * np.tanh((0.05 * (self.lr_counter - self.ideal_flag_interval)))\n",
    "            self.lr_counter = 0\n",
    "            self.missing_flags -= 1\n",
    "        self.last_reward = frame_reward\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def done(self):\n",
    "        self.rewards[-1] -= self.missing_flags * 5\n",
    "\n",
    "    def train(self, verbose=0):\n",
    "        if self.autosave is not None and self.episode % self.autosave == 0:\n",
    "            self.save(\"last.h5\")\n",
    "            print(\"Saved!\")\n",
    "        \n",
    "        #print(\"missed:\", self.missing_flags, \"flags\")\n",
    "        #self.rewards[-1] -= self.missing_flags * 5\n",
    "        sample_weights = discount_rewards(self.rewards, self.gamma)\n",
    "        self.model.fit(\n",
    "            x=np.array(self.train_x),\n",
    "            y=np.vstack(self.train_y),\n",
    "            verbose=verbose,\n",
    "            sample_weight=sample_weights\n",
    "        )\n",
    "        self.episode += 1\n",
    "        self.decay()\n",
    "\n",
    "    def total_reward(self):\n",
    "        return np.array(self.rewards).sum()\n",
    "\n",
    "    def set_autosave(self, interval):\n",
    "        self.autosave = interval\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Skier(gamma=0.95, e_decay=0.95)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_autosave(10)\n",
    "observation = env.reset()\n",
    "hist = RewardHist(100)\n",
    "agent.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = agent.action(observation, training=True)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "\n",
    "    agent.register_frame(observation)\n",
    "\n",
    "    if done:\n",
    "        agent.done()\n",
    "        total_reward = agent.total_reward()\n",
    "        hist.add(total_reward)\n",
    "\n",
    "        if agent.episode % 5 == 0:\n",
    "            print('# - = - = - = - #')\n",
    "            print(\n",
    "                f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n",
    "            hist.report()\n",
    "        break\n",
    "        agent.train(verbose=0)\n",
    "        agent.reset()\n",
    "\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = agent.action(observation)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "\n",
    "    agent.register_frame(observation)\n",
    "\n",
    "    if done:\n",
    "        agent.done()\n",
    "        total_reward = agent.total_reward()\n",
    "        print('# - = - = - = - #')\n",
    "        print(f\"Ep: {agent.episode:4}\\nTotal reward: {total_reward:.3f}\\nEpsilon: {agent.epsilon:.4f}\")\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
