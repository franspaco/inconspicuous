{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SOAgdSpoi8Md",
    "outputId": "d2b9eb37-5547-4fb5-c1b0-733c7be15684"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random, math\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "C3jpZDbFjJu8",
    "outputId": "8c387b6d-d9fd-425d-bb42-0baa94207fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 3\n",
      "['NOOP', 'RIGHT', 'LEFT']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(state_size, action_size)\n",
    "\n",
    "actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(actions)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "n_episodes = 10000\n",
    "\n",
    "print(np.random.choice([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "GeYGaLVfjhKl",
    "outputId": "60712e6b-c3bf-47e3-be11-7b6aa78e2aae"
   },
   "outputs": [],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "  \n",
    "    env.render()\n",
    "    \n",
    "    #your agent goes here\n",
    "    action = np.random.choice([0, 1,2])\n",
    "    #action = env.action_space.sample() \n",
    "    \n",
    "    observation, reward, done, info = env.step(action) \n",
    "\n",
    "    if done: \n",
    "      break;\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_reward(I, prev):\n",
    "    I = I[:, :, 1]\n",
    "    I = I[74:75, 8:152]  # Jugador 92, bandera roja 50, bandera azul 72\n",
    "    if 72 not in I and 50 not in I:\n",
    "        return 0\n",
    "    if 72 in I:\n",
    "        flags = np.where(I == 72)\n",
    "    else:\n",
    "        flags = np.where(I == 50)\n",
    "\n",
    "    player = np.where(I == 92)[1]\n",
    "\n",
    "    if len(player) == 0:\n",
    "        return 1\n",
    "\n",
    "    player = player.mean()\n",
    "\n",
    "    if len(flags[1]) == 2:\n",
    "        if player >= flags[1][0] and player <= flags[1][1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3Ww0L4w1hE4"
   },
   "source": [
    "## Define the Deep Q learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbtt8Frsjvm8"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "      \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Events that are near in time are too coralated and do not give aditional information\n",
    "        # we will use moves that are further separated in time\n",
    "        self.max_memory = 300000\n",
    "        self.memory = [] #deque(maxlen=800000)\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = (1-self.epsilon_min) / 1000\n",
    "        \n",
    "        print(self.epsilon_decay)\n",
    "        \n",
    "        self.learning_rate = 0.00025\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "        model = models.Sequential()\n",
    "        \n",
    "        model.add(layers.Conv2D(16, kernel_size = (8,8), strides=(4,4), \n",
    "                                padding = 'valid', \n",
    "                                kernel_initializer='glorot_uniform', \n",
    "                                input_shape=(72, 72, 2)))\n",
    "        model.add(layers.LeakyReLU(alpha=0.3))\n",
    "        model.add(layers.Conv2D(32, kernel_size = (4,4), strides=(2,2), \n",
    "                                padding = 'valid',\n",
    "                                kernel_initializer='glorot_uniform'))\n",
    "        model.add(layers.LeakyReLU(alpha=0.3))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(256, kernel_initializer='glorot_uniform', \n",
    "                               activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, \n",
    "                               kernel_initializer='glorot_uniform', activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer= optimizers.RMSprop(lr=self.learning_rate, rho=0.95, epsilon=0.01))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "            state, action, reward at current time\n",
    "            next_state is the state that occurs after the state-action\n",
    "            done is if the episode ended\n",
    "        '''\n",
    "        if len(self.memory) > self.max_memory:\n",
    "          self.memory.pop(0)\n",
    "          \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def action(self, state):\n",
    "        \n",
    "        # Exploration mode\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #return np.random.choice([0,2,3])\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Use what action is predicted by the model as the best choice\n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(act_values[0])\n",
    "      \n",
    "    def get_batch(self, batch_size):\n",
    "        \n",
    "        minibatch = random.sample(range(2, len(self.memory)), batch_size)\n",
    "\n",
    "        batch = [\n",
    "            (\n",
    "                np.expand_dims(np.stack(( self.memory[i-1][0], self.memory[i][0]), axis = 2), axis = 0),\n",
    "                self.memory[i][1],\n",
    "                np.sum((self.memory[i-1][2], self.memory[i][2])),\n",
    "                np.expand_dims(np.stack((self.memory[i-1][3], self.memory[i][3]), axis = 2), axis = 0),\n",
    "                self.memory[i][4]\n",
    "            ) for i in minibatch\n",
    "        ]\n",
    "        \n",
    "        return batch\n",
    "      \n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        #minibatch = random.sample(self.memory, batch_size)\n",
    "        batch = self.get_batch(batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "                \n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "    \n",
    "           \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "jyAVZOyPjzTr",
    "outputId": "8f4af8d5-ddcb-4ccf-e8bd-e63b7f1f45f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 17, 17, 16)        2064      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 17, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 32)          8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               401664    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 412,723\n",
      "Trainable params: 412,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size, action_size)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0YR2iCYd4OV"
   },
   "outputs": [],
   "source": [
    "def preprocessFrame(I):\n",
    "    I = I[::2, ::2, 1]\n",
    "    I = I[31:103, 4:76]\n",
    "    I[I == 236] = 0\n",
    "    I[I == 192] = 0\n",
    "    I[I == 214] = 0\n",
    "    I[I != 0] = 255\n",
    "    return I/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "colab_type": "code",
    "id": "eHcl0-LAj38H",
    "outputId": "0bca1dfd-e74a-40f4-f1ed-f5c23593d241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\conda3\\envs\\keras-gpu2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f07d7e4a0cc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m               \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-f8b3fda1324e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\keras-gpu2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\conda3\\envs\\keras-gpu2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\keras-gpu2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\keras-gpu2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\keras-gpu2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('SkiingDeterministic-v4')\n",
    "\n",
    "try:\n",
    "    for e in range(n_episodes):\n",
    "        \n",
    "        state = preprocessFrame(env.reset())\n",
    "        states = deque((state, state, state, state), maxlen=2)\n",
    "        states_tensor = None\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            env.render()\n",
    "            states_tensor = np.stack((states), axis = 2).reshape((1, 72, 72, 2))\n",
    "            \n",
    "            # Takes a random action from the action space of the environment\n",
    "            action = agent.action(states_tensor)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = preprocessFrame(next_state)\n",
    "            \n",
    "            # Define the reward for this problem\n",
    "            total_reward += reward\n",
    "            \n",
    "            agent.remember(state, action, total_reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            states.append(next_state)\n",
    "        \n",
    "            if len(agent.memory) > batch_size + 4:\n",
    "              agent.train(batch_size)\n",
    "        \n",
    "        if e%50 == 0:\n",
    "          print(\"Episode: {}/{}, score: {}, e: {:.9}, m: {}\".format(e, n_episodes, total_reward, agent.epsilon, len(agent.memory)))\n",
    "        \n",
    "        agent.save('max_reward_weights.hdf5')\n",
    "                \n",
    "        \n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1Ti62JUgiNm"
   },
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('SkiingDeterministic-v4')\n",
    "\n",
    "try:\n",
    "      state = env.reset()\n",
    "      state = np.reshape(state, [1, state_size])\n",
    "\n",
    "      total_reward = 0\n",
    "      done = False\n",
    "\n",
    "      while not done:\n",
    "\n",
    "          env.render()\n",
    "\n",
    "          # Takes a random action from the action space of the environment\n",
    "          action = agent.action(state)\n",
    "\n",
    "          next_state, reward, done, info = env.step(action)\n",
    "\n",
    "          total_reward += reward\n",
    "\n",
    "          next_state = np.reshape(next_state, [1, state_size])\n",
    "          state = next_state\n",
    "        \n",
    "finally:\n",
    "    env.close()       \n",
    "    show_video()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pong Deep Q Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
